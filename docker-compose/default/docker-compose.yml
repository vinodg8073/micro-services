services:
  
  # rabitmq:
  #   image: rabbitmq:3.13-management
  #   hostname: rabitmq
  #   ports: 
  #     - "5672:5672"
  #     - "15672:15672"
  #   healthcheck:
  #     test: rabbitmq-diagnostics check_port_connectivity
  #     interval: 10s
  #     timeout: 5s
  #     retries: 5  
  #     start_period: 10s
  #   extends:                                       
  #     file: common-config.yml                         
  #     service: network-deploy-services
  # redis:
  #   image: redis
  #   ports:
  #     - "6379:6379"
  #   healthcheck:
  #     test: [ "CMD-SHELL", "redis-cli ping | grep PONG" ]
  #     timeout: 10s
  #     retries: 10
  #   extends:
  #     file: common-config.yml
  #     service: network-deploy-services

  configserver:
    build: 
      context: ../../config-server/          # searches for Dockerfile and builds the image and starts the container. So no need to manually run docker build
    container_name: config-server
    ports:
      - "8091:8091"
    # depends_on:
    #   rabitmq:
    #     condition: service_healthy
    healthcheck:  # instructions to evaluate wheather CS is up with liveness and readiness or not
      test: "curl --fail --silent http://localhost:8091/actuator/health/readiness | grep UP || exit 1"
      interval: 10s
      timeout: 5s
      retries: 5  # retries for 5 times with interval of 10 seconds $ wait for 5 seconds internally for the response
      start_period: 30s  # tells to execute health check commadn after 10 seconds
    extends:                                       
      file: common-config.yml                         
      service: base_configs
    environment:
      OTEL_SERVICE_NAME: "config-server"

  laboratorydb:
    image: postgres
    container_name: laboratoryDB
    ports:
      - "5432:5432"
    environment:
      - POSTGRES_USER=root
      - POSTGRES_PASSWORD=admin
      - POSTGRES_DB=laboratoryDB
    volumes:
      - laboratoryData:/var/lib/postgres/data/
    healthcheck:
      test: ["CMD", "pg_isready", "-U", "root", "-d", "laboratoryDB"]
      interval: 30s
      timeout: 10s
      retries: 3
    extends:                                       
      file: common-config.yml                         
      service: network-deploy-services
  
  lab-user-management:
    # image: lab-user-management:0.0.1-SNAPSHOT
    build: 
      context: ../../lab-user-management/
    ports:
      - "8081:8081"
    healthcheck:  
      test: "curl --fail --silent http://localhost:8081/user/actuator/health/readiness | grep UP || exit 1"
      interval: 20s
      timeout: 5s
      retries: 5  # retries for 5 times with interval of 10 seconds $ wait for 5 seconds internally for the response
      start_period: 50s
    container_name: lab-user-management  
    environment:
      SPRING_APPLICATION_NAME: lab-user-management
      OTEL_SERVICE_NAME: "lab-user-management"  
    extends:                                       
      file: common-config.yml                         
      service: microservice_depends_on_configs
    depends_on:            # service depends on other and wait untill below services are started and is healthy then starts micro service
      lab-reports:
        condition: service_healthy

  lab-reports:
    #image: vinodg8073/lab_report:v1
    build: 
      context: ../../lab-reports/   
    container_name: lab-reports       
    ports:
      - "8080:8080"
    healthcheck:  
      test: "curl --fail --silent http://localhost:8080/reports/actuator/health/readiness | grep UP || exit 1"
      interval: 20s
      timeout: 10s
      retries: 5
      start_period: 50s
    environment:
      SPRING_APPLICATION_NAME: lab-reports
      OTEL_SERVICE_NAME: "lab-reports"
    extends:                                       
      file: common-config.yml                         
      service: microservice_depends_on_configs
    depends_on:            #When depends_on defined under common configs then the configs cannot be extended
      laboratorydb:
        condition: service_healthy      # Health check should be defined for the service or else you can also simply add "- laboratoryDB" under "depends_on:"
      eurekaserver:
        condition: service_healthy
      configserver:
        condition: service_healthy

  eurekaserver:
    build: 
      context: ../../eureka-server/
    container_name: eureka-server          
    ports:
      - "8092:8092"
    healthcheck:  # liveness and readiness 
      test: "curl --fail --silent http://localhost:8092/actuator/health/readiness | grep UP || exit 1"
      interval: 10s
      timeout: 10s
      retries: 5  
      start_period: 20s  
    extends:                                       
      file: common-config.yml                         
      service: config_server_configs  #No need of mq related properties here
    environment:
      SPRING_APPLICATION_NAME: eureka-server
      OTEL_SERVICE_NAME: eureka-server
    depends_on:           
      configserver:
        condition: service_healthy
  
  gatewayserver:
    image: gateway-server
    container_name: lab-gateway-server
    ports:
      - "8093:8093"
    environment:
      SPRING_APPLICATION_NAME: gateway-server
      OTEL_SERVICE_NAME: gateway-server
      # SPRING_DATA_REDIS_CONNECT-TIMEOUT: 2s
      # SPRING_DATA_REDIS_HOST: redis
      # SPRING_DATA_REDIS_PORT: 6379
      # SPRING_DATA_REDIS_TIMEOUT: 1s
    extends:                                       
      file: common-config.yml                         
      service: microservice_depends_on_configs
    depends_on:    
      lab-user-management:  
        condition: service_healthy
      # redis:
      #   condition: service_healthy
  
  read:
    image: grafana/loki:3.0.0
    command: "-config.file=/etc/loki/config.yaml -target=read" #command executed while creating container,target read becomes read component of loki, reads below config file from volumes which is loki-config.yaml 
    ports:
      - 3101:3100
      - 7946
      - 9095
    volumes:
      - ../observability/loki-config.yaml:/etc/loki/config.yaml  #Map local file (from system loki-config.yaml) into the container as loki/config.yml
    depends_on:
      - minio
    healthcheck:
      test: [ "CMD-SHELL", "wget --no-verbose --tries=1 --spider http://localhost:3100/ready || exit 1" ]
      interval: 10s
      timeout: 5s
      retries: 5
    networks: &loki-dns  # amp (&) indicates anchor- variale name loki-dns and assigning below values
      laboratory:
        aliases:
          - loki #alias name to use in some other configuration

  write:
    image: grafana/loki:3.0.0
    command: "-config.file=/etc/loki/config.yaml -target=write"
    ports:
      - 3102:3100
      - 7946
      - 9095
    volumes:
      - ../observability/loki-config.yaml:/etc/loki/config.yaml
    healthcheck:
      test: [ "CMD-SHELL", "wget --no-verbose --tries=1 --spider http://localhost:3100/ready || exit 1" ]
      interval: 10s
      timeout: 5s
      retries: 5
    depends_on:
      - minio
    networks:
      <<: *loki-dns  # << indicates merge and * used to refer the anchor variable
  
  alloy:
    image: grafana/alloy:v1.0.0
    volumes:
      - ../observability/alloy-local-config.yaml:/etc/alloy/config.alloy:ro  #:ro indicates readonly volume
      - /var/run/docker.sock:/var/run/docker.sock
    command: run --server.http.listen-addr=0.0.0.0:12345 --storage.path=/var/lib/alloy/data /etc/alloy/config.alloy
    ports:
      - 12345:12345
    depends_on:
      - gateway
    extends:
      file: common-config.yml                         
      service: network-deploy-services

  minio:
    image: minio/minio:RELEASE.2024-05-27T19-17-46Z
    entrypoint:  #entry point command creates directory of data/*** in local system (where docker compose file present) and store all logs w/ help of minio[In prod we can store in s3 bucket/ etc..]
      - sh
      - -euc
      - |
        mkdir -p /data/loki-data && \
        mkdir -p /data/loki-ruler && \
        minio server /data
    environment:
      - MINIO_ROOT_USER=loki
      - MINIO_ROOT_PASSWORD=supersecret
      - MINIO_PROMETHEUS_AUTH_TYPE=public
      - MINIO_UPDATE=off
    ports:
      - 9000
    volumes:
      - ./.data/minio:/data  #copy data/minio/[logs] folder to data folder in contianer
    healthcheck:
      test: [ "CMD", "curl", "-f", "http://localhost:9000/minio/health/live" ]
      interval: 15s
      timeout: 20s
      retries: 5
    extends:
      file: common-config.yml
      service: network-deploy-services
  
  backend:
    image: grafana/loki:3.0.0
    volumes:
      - ../observability/loki-config.yaml:/etc/loki/config.yaml
    ports:
      - "3100"
      - "7946"
    command: "-config.file=/etc/loki/config.yaml -target=backend -legacy-read-mode=false"
    depends_on:
      - gateway
    extends:
      file: common-config.yml
      service: network-deploy-services

  prometheus:
    image: prom/prometheus:v2.50.1
    container_name: prometheus
    ports:
      - "9090:9090"
    volumes:
      - ../observability/prometheus.yml:/etc/prometheus/prometheus.yml
    extends:
      file: common-config.yml
      service: network-deploy-services
  
  tempo:
    image: grafana/tempo:2.4.2
    container_name: tempo
    command: -config.file /etc/tempo-config.yml
    ports:
      - "3110:3100"
      - "4317:4317"
    volumes:
      - ../observability/tempo.yml:/etc/tempo-config.yml
    extends:
      file: common-config.yml
      service: network-deploy-services

  grafana:
    image: grafana/grafana:11.0.0
    environment:
      - GF_PATHS_PROVISIONING=/etc/grafana/provisioning
      - GF_AUTH_ANONYMOUS_ENABLED=true
      - GF_AUTH_ANONYMOUS_ORG_ROLE=Admin
    depends_on:
      - gateway
    entrypoint:  #using below datasources we can see the connection made in grafana
      - sh
      - -euc
      - |
        /run.sh
    ports:
      - "3000:3000"
    volumes:
      - ../observability/datasource.yml:/etc/grafana/provisioning/datasources/datasource.yml  #setup connection details provided in datasource.yml for loki and prometheus
    healthcheck:
      test: [ "CMD-SHELL", "wget --no-verbose --tries=1 --spider http://localhost:3000/api/health || exit 1" ]
      interval: 10s
      timeout: 5s
      retries: 5
    extends:
      file: common-config.yml
      service: network-deploy-services

  gateway:
    image: nginx:1.25.5
    depends_on:
      - read
      - write
    entrypoint:  # creates a config file to map the requests ex: /push to write component
      - sh
      - -euc
      - |
        cat <<EOF > /etc/nginx/nginx.conf
        user  nginx;
        worker_processes  5;  ## Default: 1

        events {
          worker_connections   1000;
        }

        http {
          resolver 127.0.0.11;

          server {
            listen             3100;

            location = / {
              return 200 'OK';
              auth_basic off;
            }

            location = /api/prom/push {
              proxy_pass       http://write:3100\$$request_uri;
            }

            location = /api/prom/tail {
              proxy_pass       http://read:3100\$$request_uri;
              proxy_set_header Upgrade \$$http_upgrade;
              proxy_set_header Connection "upgrade";
            }

            location ~ /api/prom/.* {
              proxy_pass       http://read:3100\$$request_uri;
            }

            location = /loki/api/v1/push {
              proxy_pass       http://write:3100\$$request_uri;
            }

            location = /loki/api/v1/tail {
              proxy_pass       http://read:3100\$$request_uri;
              proxy_set_header Upgrade \$$http_upgrade;
              proxy_set_header Connection "upgrade";
            }

            location ~ /loki/api/.* {
              proxy_pass       http://read:3100\$$request_uri;
            }
          }
        }
        EOF
        /docker-entrypoint.sh nginx -g "daemon off;"
    ports:
      - "3100:3100"
    healthcheck:
      test: [ "CMD", "service", "nginx", "status" ]
      interval: 10s
      timeout: 5s
      retries: 5
    extends:
      file: common-config.yml
      service: network-deploy-services

networks:
  laboratory:
    driver: "bridge"

volumes:
  laboratoryData:


# mkdir -p /etc/grafana/provisioning/datasources
#         cat <<EOF> /etc/grafana/provisioning/datasources/ds.yaml
#         apiVersion: 1
#         datasources:
#           - name: Loki
#             type: loki
#             access: proxy
#             url: http://gateway:3100
#             jsonData:
#               httpHeaderName1: "X-Scope-OrgID"
#             secureJsonData:
#               httpHeaderValue1: "tenant1"
#         EOF